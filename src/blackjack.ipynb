{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from blackjackenv_extended import BlackjackEnv\n",
    "from random_agent import RandomAgent\n",
    "from basic_strategy_agent import BasicStrategyAgent\n",
    "from backprop_agent import BackpropAgent\n",
    "from FFNN_agent import FFNNAgent\n",
    "from DQFFNN_agent import DQFFNNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = {0: 'STAND', 1:'HIT', 2:'DOUBLE', 3: 'SPLIT'}\n",
    "\n",
    "rewards = []\n",
    "\n",
    "def play_game(env, episodes, agent, collect_data=False):\n",
    "\n",
    "    for episode in tqdm(range(episodes)):\n",
    "        observation, info = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        # print(f\"Episode: {episode+1}/{episodes}\")\n",
    "        # print(\"-\"*40)\n",
    "        # print(\"hand 1\", env.player)\n",
    "        # print(\"hand 2\", env.player2)\n",
    "        # print(\"Start Observation: \", observation)\n",
    "        \n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            action = agent.get_action(observation)\n",
    "            #print(\"Action: \", KEY[action])\n",
    "\n",
    "            # save data for training\n",
    "            if collect_data:\n",
    "                agent.collect_data(observation, action)\n",
    "            \n",
    "            next_observation, reward, terminated, truncated, info = env.step(action) \n",
    "            agent.learn(observation, action, reward, next_observation, terminated or truncated)\n",
    "            observation = next_observation\n",
    "            \n",
    "            # print(\"hand 1\", env.player)\n",
    "            # print(\"hand 2\", env.player2)\n",
    "            #print(\"Observation: \", observation, \"Reward: \", reward)\n",
    "            \n",
    "\n",
    "            if terminated or truncated:\n",
    "                # print(f\"Dealer hand: \", env.dealer)\n",
    "                rewards.append(reward)\n",
    "                observation = env.reset()\n",
    "                done = True\n",
    "        \n",
    "        agent.decay_epsilon()    \n",
    "\n",
    "    if collect_data:\n",
    "        agent.save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"using gpu: \", torch.cuda.get_device_name())\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"using cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [08:43<00:00, 191.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward:  -0.296475\n",
      "Variance:  1.2807028814047645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# opdelen\n",
    "\n",
    "env = BlackjackEnv(natural=True)\n",
    "\n",
    "# Hyperparameters\n",
    "episodes = 2\n",
    "collect_data = False\n",
    "\n",
    "# Backprop hyperparameters\n",
    "input_size = 5\n",
    "output_size = 4\n",
    "hidden_size = 30\n",
    "activation_fn = nn.ReLU()\n",
    "\n",
    "# FFNN hyperparameters\n",
    "model_path = '../models/DQFFNN_model.pth'\n",
    "layers = [9,100,100]\n",
    "threshold = 0.5\n",
    "lr = 0.03\n",
    "\n",
    "# DQFFNN hyperparameters\n",
    "layers = [9,100,100]\n",
    "threshold = 3\n",
    "lr = 0.03\n",
    "epsilon = 1.0\n",
    "epsilon_decay = epsilon / episodes\n",
    "discount_factor = 0.99\n",
    "batch_size = 2\n",
    "\n",
    "\n",
    "#agent = RandomAgent(env, filename=\"random_agent\")\n",
    "#agent = BasicStrategyAgent(env, filename=\"basic_strategy_agent\")\n",
    "# agent = BackpropAgent(\n",
    "#     env, \n",
    "#     model=torch.load('../models/backprop_model.pth'),\n",
    "#     input_size=input_size, \n",
    "#     output_size=output_size, \n",
    "#     hidden_size=hidden_size, \n",
    "#     activation_fn=activation_fn,\n",
    "#     filename=\"backprop_agent\"\n",
    "#                     )\n",
    "# agent = FFNNAgent(\n",
    "#     env,\n",
    "#     model_path=model_path,\n",
    "#     device=device,\n",
    "#     layers=layers,\n",
    "#     threshold=threshold,\n",
    "#     lr=lr,\n",
    "#     filename=\"ffnn_agent\"\n",
    "#                 )\n",
    "agent = DQFFNNAgent(\n",
    "    env,\n",
    "    device=device,\n",
    "    layers=layers,\n",
    "    threshold=threshold,\n",
    "    lr=lr,\n",
    "    epsilon=epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    discount_factor=discount_factor,\n",
    "    batch_size=batch_size,\n",
    "    filename=\"dqffnn_agent\"\n",
    ")\n",
    "\n",
    "rewards = []\n",
    "\n",
    "play_game(env, episodes, agent, collect_data)\n",
    "\n",
    "average_reward = sum(rewards)/episodes\n",
    "variance = sum([((x - average_reward) ** 2) for x in rewards]) / (episodes - 1)\n",
    "\n",
    "#print(\"Rewards: \", rewards)\n",
    "print(\"Average Reward: \", average_reward)\n",
    "print(\"Variance: \", variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.model.save_model(path='../models/DQFFNN_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
